{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from evaluation import evaluate_ranking\n",
    "from interactions import Interactions\n",
    "from train import Recommender\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    dataset: str\n",
    "    data_root: str = \"data/\"\n",
    "    train_dir: str = \"/test/train.txt\"\n",
    "    test_dir: str = \"/test/test.txt\"\n",
    "    L: int = 5\n",
    "    T: int = 3\n",
    "    n_iter: int = 40\n",
    "    seed: int = 123\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    l2: float = 5e-6\n",
    "    neg_samples: int = 3\n",
    "    use_cuda: bool = True\n",
    "    save_root: str = \"checkpoints/\"\n",
    "    model_type: str = \"cnn\"\n",
    "    d: int = 50\n",
    "    block_num: int = 2\n",
    "    block_dim: list[int] = field(default_factory=lambda: [128, 256])\n",
    "    drop: float = 0.5\n",
    "    fc_dim: int = 150\n",
    "    ac_fc: str = \"tanh\"\n",
    "\n",
    "\n",
    "def load_instance(args: Args, saved_state_path: str = \"\") -> Recommender:\n",
    "    # set seed\n",
    "    set_seed(args.seed,\n",
    "             cuda=args.use_cuda)\n",
    "    # load dataset\n",
    "    train = Interactions(args.data_root+args.dataset+args.train_dir)\n",
    "    # transform triplets to sequence representation\n",
    "    train.to_sequence(args.L, args.T)\n",
    "\n",
    "    test = Interactions(args.data_root+args.dataset+args.test_dir,\n",
    "                        user_map=train.user_map,\n",
    "                        item_map=train.item_map)\n",
    "\n",
    "    # Instantiate Model\n",
    "    model = Recommender(args)\n",
    "\n",
    "    # Initialize the Model\n",
    "    model._initialize(train)\n",
    "\n",
    "    if saved_state_path:\n",
    "        saved_state = torch.load(saved_state_path, map_location=model._device)\n",
    "\n",
    "        # If provided, load the pytorch state\n",
    "        if \"state_dict\" in saved_state:\n",
    "            model._net.load_state_dict(saved_state[\"state_dict\"])\n",
    "        if \"optimizer\" in saved_state:\n",
    "            model._optimizer.load_state_dict(saved_state[\"optimizer\"])\n",
    "\n",
    "    return model, train, test\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    map: float\n",
    "    prec1: float\n",
    "    prec5: float\n",
    "    prec10: float\n",
    "    recall1: float\n",
    "    recall5: float\n",
    "    recall10: float\n",
    "\n",
    "\n",
    "def evaluate(model, test, train) -> EvaluationResult:\n",
    "    precision, recall, mean_aps = evaluate_ranking(\n",
    "        model, test, train, k=[1, 5, 10])\n",
    "\n",
    "    return EvaluationResult(mean_aps, *[np.mean(p) for p in precision], *[np.mean(r) for r in recall])\n",
    "\n",
    "\n",
    "def display_result(result: EvaluationResult) -> None:\n",
    "    md = \"| Metric | Value |\\n| --- | --- |\\n\"\n",
    "    md += f\"| MAP | {result.map:.4f} |\\n\"\n",
    "    md += f\"| Precision (@1) | {result.prec1:.4f} |\\n\"\n",
    "    md += f\"| Precision (@5) | {result.prec5:.4f} |\\n\"\n",
    "    md += f\"| Precision (@10) | {result.prec10:.4f} |\\n\"\n",
    "    md += f\"| Recall (@1) | {result.recall1:.4f} |\\n\"\n",
    "    md += f\"| Recall (@5) | {result.recall5:.4f} |\\n\"\n",
    "    md += f\"| Recall (@10) | {result.recall10:.4f} |\\n\"\n",
    "    display(Markdown(md))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train, test = load_instance(Args(dataset=\"ml1m\", use_cuda=False), saved_state_path=\"checkpoints/ml1m/ml1m_123.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result = evaluate(model, train, test)\n",
    "display_result(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
